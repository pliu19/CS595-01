{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll work on transparency of Bernoulli naive Bayes. You need a \"binary classification\" dataset (two classes), and you need all features to be either already binary or you need to binarize them yourself. Do NOT use the \"binarize\" field in the constructor of BernoulliNB. Each continuous feature should be binarized based on whether it is larger or smaller than its mean. Categorical features that have more than two possible values should use one-hot encoding.  The new features should have proper names according to the binarization. Ask me if you are uncertain about any of this.\n",
    "\n",
    "1. Choose a binary classification dataset. (The same rules in Assignment 3 applies, except there is one additional rule this time: it has to be a classification dataset).\n",
    "\n",
    "2. Create a Jupyter notebook. Name it assignment8. It should have several sections. The first section should describe the dataset. The second section should load the dataset. The third section should binarize all the features.\n",
    "\n",
    "3. If the dataset already comes in train-test split, great. If not, create a train-test split, using 2/3 for train and 1/3 for test.\n",
    "\n",
    "4. Train a BernouilliNB classifier on the train split; use the default parameters.\n",
    "\n",
    "5. For the following types of objects, create a section, and print a) the total positive log-evidence,  b) the total negative log-evidence, c) probability distribution, d) top 3 features values that contribute most to the positive evidence, e) top 3 feature values that contribute the most to the negative evidence. Print this information on the following object types\n",
    "    - The most positive object with respect to the probabilities.\n",
    "    - The most negative object with respect to the probabilities.\n",
    "    - The object that has the largest positive evidence.\n",
    "    - The object that has the largest (in magnitude) negative evidence.\n",
    "    - The most uncertain object (the probabilities are closest to 0.5)\n",
    "\n",
    "Upload the notebook to your 595 github repository.\n",
    "\n",
    "Submit the link using blackboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The data is IMDB - review dataset. It's available from the website \"http://ai.stanford.edu/~amaas/data/sentiment/\". \n",
    "\n",
    "The review is binary case which could be positive/negative review. It provides 25,000 reviews for training, and 25,000 reviews for testing. The distribution of class in training and testing is balanced, which means it has 12500 positive reviews and 12500 negative reviews in both training and testing.\n",
    "\n",
    "In this assignment, we will use countvectorizer which means each token feature will be binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_imdb(path):\n",
    "\n",
    "    print(\"Loading the imdb reviews data\")\n",
    "\n",
    "    train_neg_files = glob.glob(path+\"/train/neg/*.txt\")\n",
    "    train_pos_files = glob.glob(path+\"/train/pos/*.txt\")\n",
    "\n",
    "    train_corpus = []\n",
    "    y_train = []\n",
    "    \n",
    "    for tnf in train_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        train_corpus.append(line)\n",
    "        y_train.append(0)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    for tpf in train_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        train_corpus.append(line)\n",
    "        y_train.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Training Data loaded.\")\n",
    "    print()\n",
    "    \n",
    "    test_neg_files = glob.glob(path+\"/test/neg/*.txt\")\n",
    "    test_pos_files = glob.glob(path+\"/test/pos/*.txt\")\n",
    "\n",
    "    test_corpus = []\n",
    "\n",
    "    y_test = []\n",
    "\n",
    "    for tnf in test_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        test_corpus.append(f.read())\n",
    "        y_test.append(0)\n",
    "        f.close()\n",
    "\n",
    "    for tpf in test_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        test_corpus.append(f.read())\n",
    "        y_test.append(1)\n",
    "        f.close()\n",
    "\n",
    "    print(\"Testing Data loaded.\")\n",
    "    print()\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    return train_corpus, test_corpus, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_path = \"../aclImdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the imdb reviews data\n",
      "Training Data loaded.\n",
      "\n",
      "Testing Data loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_corpus, test_corpus, y_train, y_test = load_imdb(imdb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize and vectorize the data into matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, max_df=1.0, binary=True)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_corpus)\n",
    "\n",
    "X_test = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 27272)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82904"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analysis of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bias(clf):\n",
    "    return clf.class_log_prior_[1] - clf.class_log_prior_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It should be zero since the data is balanced as 50% vs 50%\n",
    "\n",
    "get_bias(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In this case, we only need to calculate the evidence (negative and positive) for all the features \n",
    "\n",
    "def evidences(clf, X):\n",
    "    \n",
    "    X = X.todense()\n",
    "    \n",
    "    X_presence = X.copy()\n",
    "    \n",
    "    X_absence = 1 - X\n",
    "        \n",
    "    # p(x=absence|y) = 1 - p(x=presence|y)    \n",
    "    # ln[p(x=absence|y)] = ln[1 - e^ln[p=presence|y]]\n",
    "    absence_log_prob_ = np.log(1 - np.exp(clf.feature_log_prob_))\n",
    "    \n",
    "    # log-ratio => ln[p(x=presence|y=true)/p(x=absence|y=false)] = ln[p(x=presence|y=true)] - ln[p(x=absence|y=true)]\n",
    "    presence_log_ratios = clf.feature_log_prob_[1] - clf.feature_log_prob_[0]\n",
    "    \n",
    "    # log-ratio => ln[p(x=absense|y=true)/p(x=presence|y=false)] = ln[p(x=absense|y=true)] - ln[p(x=presence|y=true)]\n",
    "    absence_log_ratios = absence_log_prob_[1] - absence_log_prob_[0]\n",
    "    \n",
    "    presence_neg_log_ratios = presence_log_ratios * (presence_log_ratios<0)\n",
    "    presence_pos_log_ratios = presence_log_ratios * (presence_log_ratios>0)\n",
    "    \n",
    "    absence_neg_log_ratios = absence_log_ratios * (absence_log_ratios<0)\n",
    "    absence_pos_log_ratios = absence_log_ratios * (absence_log_ratios>0)\n",
    "    \n",
    "    p_neg_evi = np.dot(X_presence, presence_neg_log_ratios)\n",
    "    p_pos_evi = np.dot(X_presence, presence_pos_log_ratios)\n",
    "    \n",
    "    a_neg_evi = np.dot(X_absence, absence_neg_log_ratios)\n",
    "    a_pos_evi = np.dot(X_absence, absence_pos_log_ratios)\n",
    "    \n",
    "    return p_neg_evi.item(0), p_pos_evi.item(0), a_neg_evi.item(0), a_pos_evi.item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_neg_evidence(clf, x_vector):\n",
    "    \n",
    "    p_neg_evi, p_pos_evi, a_neg_evi, a_pos_evi = evidences(clf, x_vector)\n",
    "    \n",
    "    return p_neg_evi + a_neg_evi, p_pos_evi + a_pos_evi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evidence = []\n",
    "\n",
    "for i in X_test:\n",
    "    evidence.append(pos_neg_evidence(clf, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence = np.asarray(evidence)\n",
    "\n",
    "evidence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-36.61238513,  30.5457996 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_evidence = evidence[:, 0]\n",
    "pos_evidence = evidence[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a) the total positive log-evidence \n",
    "# b) the total negative log-evidence\n",
    "# c) probability distribution\n",
    "# d) top 3 features values that contribute most to the positive evidence\n",
    "# e) top 3 feature values that contribute the most to the negative evidence. \n",
    "# Print this information on the following object types\n",
    "\n",
    "\n",
    "def print_infor(X_vector, index, evidence, proba, clf, vec):\n",
    "    \n",
    "    neg_evi = evidence[:, 0]\n",
    "    pos_evi = evidence[:, 1]\n",
    "    \n",
    "    print(\"The total positive log-evidence: %s\" %str(pos_evi[index]))\n",
    "    print(\"The total negative log-evidence: %s\" %str(neg_evi[index]))\n",
    "           \n",
    "    print(\"The probability distribution: %s\" %str(proba[index]))\n",
    "    \n",
    "    top_features(X_vector, clf, vec)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_features(X_vector, clf, vec):\n",
    "    \n",
    "    X_vector = X_vector.todense()\n",
    "    \n",
    "    X_presence = X_vector.copy()\n",
    "    \n",
    "    X_absence = 1 - X_vector\n",
    "        \n",
    "    # p(x=absence|y) = 1 - p(x=presence|y)    \n",
    "    # ln[p(x=absence|y)] = ln[1 - e^ln[p=presence|y]]\n",
    "    absence_log_prob_ = np.log(1 - np.exp(clf.feature_log_prob_))\n",
    "    \n",
    "    # log-ratio => ln[p(x=presence|y=true)/p(x=absence|y=false)] = ln[p(x=presence|y=true)] - ln[p(x=absence|y=true)]\n",
    "    presence_log_ratios = clf.feature_log_prob_[1] - clf.feature_log_prob_[0]\n",
    "    \n",
    "    # log-ratio => ln[p(x=absense|y=true)/p(x=presence|y=false)] = ln[p(x=absense|y=true)] - ln[p(x=presence|y=true)]\n",
    "    absence_log_ratios = absence_log_prob_[1] - absence_log_prob_[0]\n",
    "    \n",
    "    presence_neg_log_ratios = presence_log_ratios * (presence_log_ratios<0)\n",
    "    presence_pos_log_ratios = presence_log_ratios * (presence_log_ratios>0)\n",
    "    \n",
    "    absence_neg_log_ratios = absence_log_ratios * (absence_log_ratios<0)\n",
    "    absence_pos_log_ratios = absence_log_ratios * (absence_log_ratios>0)\n",
    "    \n",
    "    p_neg_evi = np.multiply(X_presence, presence_neg_log_ratios)\n",
    "    p_pos_evi = np.multiply(X_presence, presence_pos_log_ratios)\n",
    "    \n",
    "    a_neg_evi = np.multiply(X_absence, absence_neg_log_ratios)\n",
    "    a_pos_evi = np.multiply(X_absence, absence_pos_log_ratios)\n",
    "    \n",
    "    positive = np.add(p_pos_evi, a_pos_evi).A1\n",
    "    negative = np.add(p_neg_evi, a_neg_evi).A1\n",
    "    \n",
    "    total = np.add(positive, negative)\n",
    "        \n",
    "    features_name = vec.get_feature_names()\n",
    "    \n",
    "    top_positive_idx = np.argsort(positive)[::-1][:3]\n",
    "    print(\"\")\n",
    "    print(\"Top 3 features values that contribute the most to the positive evidence\")\n",
    "    \n",
    "    for idx in top_positive_idx:\n",
    "        print(\"Feature names: %s \\t\\t Evidence: %s\" %(features_name[idx], str(positive[idx])))\n",
    "        \n",
    "    top_negative_idx = np.argsort(negative)[:3]\n",
    "    print(\"\")\n",
    "    print(\"Top 3 features values that contribute the most to the negative evidence\")\n",
    "    for idx in top_negative_idx:\n",
    "        print(\"Feature names: %s \\t\\t Evidence: %s\" %(features_name[idx], str(negative[idx])))\n",
    "            \n",
    "#     print(np.count_nonzero(p_pos_evi))\n",
    "#     print(np.count_nonzero(a_pos_evi))\n",
    "#     print(np.count_nonzero(positive))\n",
    "    \n",
    "    return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The most positive object with respect to the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.argmax(y_proba[:,1])\n",
    "\n",
    "idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total positive log-evidence: 126.092040458\n",
      "The total negative log-evidence: -64.9802025856\n",
      "The probability distribution: [  2.88048775e-27   1.00000000e+00]\n",
      "\n",
      "Top 3 features values that contribute the most to the positive evidence\n",
      "Feature names: pinjar \t\t Evidence: 2.19722457734\n",
      "Feature names: maintained \t\t Evidence: 1.68639895357\n",
      "Feature names: sadness \t\t Evidence: 1.6635051337\n",
      "\n",
      "Top 3 features values that contribute the most to the negative evidence\n",
      "Feature names: existent \t\t Evidence: -1.99720344344\n",
      "Feature names: adage \t\t Evidence: -1.94591014906\n",
      "Feature names: bearings \t\t Evidence: -1.79175946923\n"
     ]
    }
   ],
   "source": [
    "print_infor(X_test[idx], idx, evidence, y_proba, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. The most negative object with respect to the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10505"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.argmin(y_proba[:,1])\n",
    "\n",
    "idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total positive log-evidence: 48.0671904571\n",
      "The total negative log-evidence: -137.652105367\n",
      "The probability distribution: [  1.00000000e+00   1.24098289e-39]\n",
      "\n",
      "Top 3 features values that contribute the most to the positive evidence\n",
      "Feature names: sloth \t\t Evidence: 1.38629436112\n",
      "Feature names: ferocious \t\t Evidence: 1.20397280433\n",
      "Feature names: joy \t\t Evidence: 0.962571484438\n",
      "\n",
      "Top 3 features values that contribute the most to the negative evidence\n",
      "Feature names: manos \t\t Evidence: -3.33220451018\n",
      "Feature names: unwatchable \t\t Evidence: -2.9856819377\n",
      "Feature names: waster \t\t Evidence: -2.63905732962\n"
     ]
    }
   ],
   "source": [
    "print_infor(X_test[idx], idx, evidence, y_proba, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The object that has the largest positive evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-163.3492882   194.84244513]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18112"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.argmax(pos_evidence)\n",
    "\n",
    "print(evidence[idx])\n",
    "\n",
    "idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total positive log-evidence: 194.842445129\n",
      "The total negative log-evidence: -163.349288197\n",
      "The probability distribution: [  2.10230502e-14   1.00000000e+00]\n",
      "\n",
      "Top 3 features values that contribute the most to the positive evidence\n",
      "Feature names: quibble \t\t Evidence: 3.04452243772\n",
      "Feature names: genesis \t\t Evidence: 2.07944154168\n",
      "Feature names: beckett \t\t Evidence: 2.07944154168\n",
      "\n",
      "Top 3 features values that contribute the most to the negative evidence\n",
      "Feature names: clowns \t\t Evidence: -2.63905732962\n",
      "Feature names: lamest \t\t Evidence: -2.25129179861\n",
      "Feature names: worst \t\t Evidence: -2.19282896586\n"
     ]
    }
   ],
   "source": [
    "print_infor(X_test[idx], idx, evidence, y_proba, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. The object that has the largest (in magnitude) negative evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-163.3492882   194.84244513]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18112"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.argmin(neg_evidence)\n",
    "\n",
    "print(evidence[idx])\n",
    "\n",
    "idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total positive log-evidence: 194.842445129\n",
      "The total negative log-evidence: -163.349288197\n",
      "The probability distribution: [  2.10230502e-14   1.00000000e+00]\n",
      "\n",
      "Top 3 features values that contribute the most to the positive evidence\n",
      "Feature names: quibble \t\t Evidence: 3.04452243772\n",
      "Feature names: genesis \t\t Evidence: 2.07944154168\n",
      "Feature names: beckett \t\t Evidence: 2.07944154168\n",
      "\n",
      "Top 3 features values that contribute the most to the negative evidence\n",
      "Feature names: clowns \t\t Evidence: -2.63905732962\n",
      "Feature names: lamest \t\t Evidence: -2.25129179861\n",
      "Feature names: worst \t\t Evidence: -2.19282896586\n"
     ]
    }
   ],
   "source": [
    "print_infor(X_test[idx], idx, evidence, y_proba, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. The most uncertain object (the probabilities are closest to 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-33.01112016  33.01051477]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15608"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncerts = np.min(y_proba, axis=1)\n",
    "\n",
    "idx = np.argmax(uncerts)\n",
    "\n",
    "print(evidence[idx])\n",
    "\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total positive log-evidence: 33.0105147676\n",
      "The total negative log-evidence: -33.0111201556\n",
      "The probability distribution: [ 0.50015135  0.49984865]\n",
      "\n",
      "Top 3 features values that contribute the most to the positive evidence\n",
      "Feature names: donovan \t\t Evidence: 1.29928298413\n",
      "Feature names: joy \t\t Evidence: 0.962571484438\n",
      "Feature names: carol \t\t Evidence: 0.826678573184\n",
      "\n",
      "Top 3 features values that contribute the most to the negative evidence\n",
      "Feature names: poor \t\t Evidence: -1.20207912055\n",
      "Feature names: cow \t\t Evidence: -0.944461608841\n",
      "Feature names: slammer \t\t Evidence: -0.916290731874\n"
     ]
    }
   ],
   "source": [
    "print_infor(X_test[idx], idx, evidence, y_proba, clf, vectorizer)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
